# NDH å¯æŠ½æ›æ™‚åºæ•¸æ“šåº«æ¶æ§‹è¨­è¨ˆ

**ä½œè€…**: æ—å¿—éŒš (Chih Cheng Lin, Michael Lin)  
**æ—¥æœŸ**: 2025 å¹´ 10 æœˆ 12 æ—¥  
**ç‰ˆæœ¬**: 1.0

---

## åŸ·è¡Œæ‘˜è¦

æœ¬æ–‡æª”è¨­è¨ˆä¸€å€‹**å¯æŠ½æ›æ™‚åºæ•¸æ“šåº«çš„ NDH æ¶æ§‹**,å…è¨±æ ¹æ“šä¸åŒå ´åŸŸçš„éœ€æ±‚éˆæ´»åˆ‡æ›åº•å±¤æ™‚åºæ•¸æ“šåº«(InfluxDB, TDengine, TimescaleDB, QuestDB ç­‰),è€Œç„¡éœ€ä¿®æ”¹ä¸Šå±¤æ‡‰ç”¨ä»£ç¢¼ã€‚

**æ ¸å¿ƒè¨­è¨ˆåŸå‰‡**:
1. **æŠ½è±¡å±¤éš”é›¢**: å®šç¾©çµ±ä¸€çš„æ™‚åºæ•¸æ“šåº«æ¥å£
2. **é©é…å™¨æ¨¡å¼**: ç‚ºæ¯ç¨®æ•¸æ“šåº«å¯¦ç¾é©é…å™¨
3. **é…ç½®é©…å‹•**: é€šéé…ç½®æ–‡ä»¶åˆ‡æ›æ•¸æ“šåº«
4. **é›¶åœæ©Ÿåˆ‡æ›**: æ”¯æ´åœ¨ç·šé·ç§»å’Œé›™å¯«

**æ”¯æ´çš„æ•¸æ“šåº«**:
- âœ… InfluxDB 2.x/3.x
- âœ… TDengine 3.x
- âœ… TimescaleDB 2.x
- âœ… QuestDB
- âœ… å¯æ“´å±•æ”¯æ´å…¶ä»–æ•¸æ“šåº«

**é æœŸæ•ˆç›Š**:
- ğŸ¯ **éˆæ´»æ€§**: æ ¹æ“šå ´åŸŸéœ€æ±‚é¸æ“‡æœ€é©åˆçš„æ•¸æ“šåº«
- ğŸ¯ **é¢¨éšªç®¡ç†**: é™ä½ä¾›æ‡‰éˆé¢¨éšª,é¿å…å» å•†é–å®š
- ğŸ¯ **å¹³æ»‘é·ç§»**: æ”¯æ´é›¶åœæ©Ÿåœ¨ç·šé·ç§»
- ğŸ¯ **æˆæœ¬å„ªåŒ–**: åœ¨æ€§èƒ½ã€æˆæœ¬ã€å®‰å…¨ä¹‹é–“éˆæ´»å¹³è¡¡

---

## 1. æ¶æ§‹ç¸½è¦½

### 1.1 åˆ†å±¤æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ‡‰ç”¨å±¤ (Application Layer)                      â”‚
â”‚  Digital Twin Apps, Dashboards, Analytics, AI/ML            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              API å±¤ (API Layer)                              â”‚
â”‚  RESTful API, GraphQL, WebSocket                            â”‚
â”‚  - /api/v1/timeseries/query                                 â”‚
â”‚  - /api/v1/timeseries/write                                 â”‚
â”‚  - /api/v1/timeseries/aggregate                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æ™‚åºæ•¸æ“šåº«æŠ½è±¡å±¤ (TSDB Abstraction Layer) â­         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  çµ±ä¸€æ¥å£ (Unified Interface)                          â”‚  â”‚
â”‚  â”‚  - ITSDBWriter: write(), batch_write()                â”‚  â”‚
â”‚  â”‚  - ITSDBReader: query(), aggregate(), stream()        â”‚  â”‚
â”‚  â”‚  - ITSDBAdmin: create_table(), drop_table(), backup() â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         é©é…å™¨å±¤ (Adapter Layer) â­                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ InfluxDB â”‚ â”‚ TDengine â”‚ â”‚Timescale â”‚ â”‚ QuestDB  â”‚      â”‚
â”‚  â”‚ Adapter  â”‚ â”‚ Adapter  â”‚ â”‚   DB     â”‚ â”‚ Adapter  â”‚      â”‚
â”‚  â”‚          â”‚ â”‚          â”‚ â”‚ Adapter  â”‚ â”‚          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       â”‚  æ•¸æ“šåº«å±¤ (Database Layer)  â”‚            â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ InfluxDB â”‚ â”‚ TDengine â”‚ â”‚Timescale â”‚ â”‚ QuestDB  â”‚      â”‚
â”‚  â”‚ Cluster  â”‚ â”‚ Cluster  â”‚ â”‚   DB     â”‚ â”‚ Cluster  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒçµ„ä»¶

#### 1.2.1 æ™‚åºæ•¸æ“šåº«æŠ½è±¡å±¤ (TSDB Abstraction Layer)

**è·è²¬**:
- å®šç¾©çµ±ä¸€çš„æ™‚åºæ•¸æ“šåº«æ¥å£
- ç®¡ç†é©é…å™¨ç”Ÿå‘½é€±æœŸ
- æä¾›é…ç½®é©…å‹•çš„æ•¸æ“šåº«åˆ‡æ›
- æ”¯æ´å¤šæ•¸æ“šåº«ä¸¦è¡Œé‹è¡Œ(é›™å¯«)

**æ ¸å¿ƒæ¥å£**:
```python
# tsdb_interface.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Iterator
from datetime import datetime

class ITSDBWriter(ABC):
    """æ™‚åºæ•¸æ“šåº«å¯«å…¥æ¥å£"""
    
    @abstractmethod
    async def write(self, measurement: str, tags: Dict[str, str], 
                   fields: Dict[str, Any], timestamp: datetime) -> bool:
        """å¯«å…¥å–®æ¢æ•¸æ“š"""
        pass
    
    @abstractmethod
    async def batch_write(self, points: List[Dict]) -> bool:
        """æ‰¹é‡å¯«å…¥æ•¸æ“š"""
        pass
    
    @abstractmethod
    async def write_line_protocol(self, lines: List[str]) -> bool:
        """ä½¿ç”¨ Line Protocol å¯«å…¥"""
        pass


class ITSDBReader(ABC):
    """æ™‚åºæ•¸æ“šåº«è®€å–æ¥å£"""
    
    @abstractmethod
    async def query(self, query: str, params: Dict = None) -> List[Dict]:
        """åŸ·è¡ŒæŸ¥è©¢"""
        pass
    
    @abstractmethod
    async def query_range(self, measurement: str, tags: Dict, 
                         start: datetime, end: datetime, 
                         fields: List[str] = None) -> List[Dict]:
        """ç¯„åœæŸ¥è©¢"""
        pass
    
    @abstractmethod
    async def aggregate(self, measurement: str, tags: Dict,
                       start: datetime, end: datetime,
                       aggregation: str, interval: str) -> List[Dict]:
        """èšåˆæŸ¥è©¢"""
        pass
    
    @abstractmethod
    async def stream(self, measurement: str, tags: Dict,
                    callback: callable) -> None:
        """ä¸²æµæŸ¥è©¢"""
        pass


class ITSDBAdmin(ABC):
    """æ™‚åºæ•¸æ“šåº«ç®¡ç†æ¥å£"""
    
    @abstractmethod
    async def create_measurement(self, name: str, schema: Dict) -> bool:
        """å‰µå»º measurement/table"""
        pass
    
    @abstractmethod
    async def drop_measurement(self, name: str) -> bool:
        """åˆªé™¤ measurement/table"""
        pass
    
    @abstractmethod
    async def list_measurements(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰ measurements"""
        pass
    
    @abstractmethod
    async def backup(self, path: str) -> bool:
        """å‚™ä»½æ•¸æ“š"""
        pass
    
    @abstractmethod
    async def restore(self, path: str) -> bool:
        """æ¢å¾©æ•¸æ“š"""
        pass
    
    @abstractmethod
    async def health_check(self) -> Dict:
        """å¥åº·æª¢æŸ¥"""
        pass


class ITSDBAdapter(ITSDBWriter, ITSDBReader, ITSDBAdmin):
    """å®Œæ•´çš„æ™‚åºæ•¸æ“šåº«é©é…å™¨æ¥å£"""
    
    @abstractmethod
    async def connect(self, config: Dict) -> bool:
        """é€£æ¥æ•¸æ“šåº«"""
        pass
    
    @abstractmethod
    async def disconnect(self) -> bool:
        """æ–·é–‹é€£æ¥"""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> Dict:
        """ç²å–æ•¸æ“šåº«èƒ½åŠ›"""
        pass
```

#### 1.2.2 é©é…å™¨ç®¡ç†å™¨ (Adapter Manager)

**è·è²¬**:
- è¼‰å…¥å’Œç®¡ç†é©é…å™¨
- æ ¹æ“šé…ç½®é¸æ“‡é©é…å™¨
- æ”¯æ´å¤šé©é…å™¨ä¸¦è¡Œé‹è¡Œ
- æä¾›é©é…å™¨åˆ‡æ›åŠŸèƒ½

**å¯¦ç¾**:
```python
# adapter_manager.py

from typing import Dict, List, Optional
import importlib
import logging

class TSDBAdapterManager:
    """æ™‚åºæ•¸æ“šåº«é©é…å™¨ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.adapters: Dict[str, ITSDBAdapter] = {}
        self.primary_adapter: Optional[ITSDBAdapter] = None
        self.secondary_adapters: List[ITSDBAdapter] = []
        self.logger = logging.getLogger(__name__)
    
    async def initialize(self):
        """åˆå§‹åŒ–é©é…å™¨"""
        # è¼‰å…¥ä¸»é©é…å™¨
        primary_config = self.config['primary']
        self.primary_adapter = await self._load_adapter(
            primary_config['type'], 
            primary_config
        )
        
        # è¼‰å…¥æ¬¡è¦é©é…å™¨(ç”¨æ–¼é›™å¯«)
        if 'secondary' in self.config:
            for secondary_config in self.config['secondary']:
                adapter = await self._load_adapter(
                    secondary_config['type'],
                    secondary_config
                )
                self.secondary_adapters.append(adapter)
        
        self.logger.info(f"Initialized primary adapter: {primary_config['type']}")
        self.logger.info(f"Initialized {len(self.secondary_adapters)} secondary adapters")
    
    async def _load_adapter(self, adapter_type: str, config: Dict) -> ITSDBAdapter:
        """è¼‰å…¥é©é…å™¨"""
        # å‹•æ…‹è¼‰å…¥é©é…å™¨æ¨¡çµ„
        module_name = f"ndh.adapters.{adapter_type}_adapter"
        class_name = f"{adapter_type.capitalize()}Adapter"
        
        try:
            module = importlib.import_module(module_name)
            adapter_class = getattr(module, class_name)
            adapter = adapter_class()
            await adapter.connect(config)
            
            self.adapters[adapter_type] = adapter
            return adapter
        except Exception as e:
            self.logger.error(f"Failed to load adapter {adapter_type}: {e}")
            raise
    
    async def write(self, measurement: str, tags: Dict, 
                   fields: Dict, timestamp: datetime) -> bool:
        """å¯«å…¥æ•¸æ“š(æ”¯æ´é›™å¯«)"""
        # å¯«å…¥ä¸»æ•¸æ“šåº«
        success = await self.primary_adapter.write(
            measurement, tags, fields, timestamp
        )
        
        # å¯«å…¥æ¬¡è¦æ•¸æ“šåº«(éé˜»å¡)
        for adapter in self.secondary_adapters:
            try:
                await adapter.write(measurement, tags, fields, timestamp)
            except Exception as e:
                self.logger.warning(f"Secondary write failed: {e}")
        
        return success
    
    async def query(self, query: str, params: Dict = None) -> List[Dict]:
        """æŸ¥è©¢æ•¸æ“š(åƒ…å¾ä¸»æ•¸æ“šåº«)"""
        return await self.primary_adapter.query(query, params)
    
    async def switch_primary(self, new_primary_type: str):
        """åˆ‡æ›ä¸»æ•¸æ“šåº«"""
        if new_primary_type not in self.adapters:
            raise ValueError(f"Adapter {new_primary_type} not loaded")
        
        old_primary = self.primary_adapter
        self.primary_adapter = self.adapters[new_primary_type]
        
        # å°‡èˆŠçš„ä¸»æ•¸æ“šåº«åŠ å…¥æ¬¡è¦æ•¸æ“šåº«åˆ—è¡¨
        if old_primary not in self.secondary_adapters:
            self.secondary_adapters.append(old_primary)
        
        self.logger.info(f"Switched primary adapter to {new_primary_type}")
    
    async def health_check(self) -> Dict:
        """å¥åº·æª¢æŸ¥"""
        result = {
            'primary': await self.primary_adapter.health_check(),
            'secondary': []
        }
        
        for adapter in self.secondary_adapters:
            try:
                health = await adapter.health_check()
                result['secondary'].append(health)
            except Exception as e:
                result['secondary'].append({'status': 'error', 'error': str(e)})
        
        return result
```

---

## 2. é©é…å™¨å¯¦ç¾

### 2.1 InfluxDB é©é…å™¨

```python
# influxdb_adapter.py

from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
from typing import Dict, List, Any
from datetime import datetime
import logging

class InfluxdbAdapter(ITSDBAdapter):
    """InfluxDB é©é…å™¨"""
    
    def __init__(self):
        self.client: Optional[InfluxDBClient] = None
        self.write_api = None
        self.query_api = None
        self.bucket = None
        self.org = None
        self.logger = logging.getLogger(__name__)
    
    async def connect(self, config: Dict) -> bool:
        """é€£æ¥ InfluxDB"""
        try:
            self.client = InfluxDBClient(
                url=config['url'],
                token=config['token'],
                org=config['org']
            )
            self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
            self.query_api = self.client.query_api()
            self.bucket = config['bucket']
            self.org = config['org']
            
            # å¥åº·æª¢æŸ¥
            health = self.client.health()
            if health.status == "pass":
                self.logger.info("Connected to InfluxDB successfully")
                return True
            else:
                raise Exception(f"InfluxDB health check failed: {health.message}")
        except Exception as e:
            self.logger.error(f"Failed to connect to InfluxDB: {e}")
            raise
    
    async def disconnect(self) -> bool:
        """æ–·é–‹é€£æ¥"""
        if self.client:
            self.client.close()
            self.logger.info("Disconnected from InfluxDB")
        return True
    
    async def write(self, measurement: str, tags: Dict[str, str],
                   fields: Dict[str, Any], timestamp: datetime) -> bool:
        """å¯«å…¥å–®æ¢æ•¸æ“š"""
        try:
            point = Point(measurement)
            for tag_key, tag_value in tags.items():
                point = point.tag(tag_key, tag_value)
            for field_key, field_value in fields.items():
                point = point.field(field_key, field_value)
            point = point.time(timestamp)
            
            self.write_api.write(bucket=self.bucket, org=self.org, record=point)
            return True
        except Exception as e:
            self.logger.error(f"Write failed: {e}")
            return False
    
    async def batch_write(self, points: List[Dict]) -> bool:
        """æ‰¹é‡å¯«å…¥"""
        try:
            influx_points = []
            for point_data in points:
                point = Point(point_data['measurement'])
                for tag_key, tag_value in point_data.get('tags', {}).items():
                    point = point.tag(tag_key, tag_value)
                for field_key, field_value in point_data.get('fields', {}).items():
                    point = point.field(field_key, field_value)
                point = point.time(point_data['timestamp'])
                influx_points.append(point)
            
            self.write_api.write(bucket=self.bucket, org=self.org, record=influx_points)
            return True
        except Exception as e:
            self.logger.error(f"Batch write failed: {e}")
            return False
    
    async def query(self, query: str, params: Dict = None) -> List[Dict]:
        """åŸ·è¡Œ Flux æŸ¥è©¢"""
        try:
            tables = self.query_api.query(query, org=self.org)
            results = []
            for table in tables:
                for record in table.records:
                    results.append(record.values)
            return results
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            raise
    
    async def query_range(self, measurement: str, tags: Dict,
                         start: datetime, end: datetime,
                         fields: List[str] = None) -> List[Dict]:
        """ç¯„åœæŸ¥è©¢"""
        # æ§‹å»º Flux æŸ¥è©¢
        flux_query = f'''
        from(bucket: "{self.bucket}")
          |> range(start: {start.isoformat()}, stop: {end.isoformat()})
          |> filter(fn: (r) => r._measurement == "{measurement}")
        '''
        
        # æ·»åŠ  tag éæ¿¾
        for tag_key, tag_value in tags.items():
            flux_query += f'  |> filter(fn: (r) => r.{tag_key} == "{tag_value}")\n'
        
        # æ·»åŠ  field éæ¿¾
        if fields:
            field_filter = ' or '.join([f'r._field == "{f}"' for f in fields])
            flux_query += f'  |> filter(fn: (r) => {field_filter})\n'
        
        return await self.query(flux_query)
    
    async def aggregate(self, measurement: str, tags: Dict,
                       start: datetime, end: datetime,
                       aggregation: str, interval: str) -> List[Dict]:
        """èšåˆæŸ¥è©¢"""
        flux_query = f'''
        from(bucket: "{self.bucket}")
          |> range(start: {start.isoformat()}, stop: {end.isoformat()})
          |> filter(fn: (r) => r._measurement == "{measurement}")
        '''
        
        for tag_key, tag_value in tags.items():
            flux_query += f'  |> filter(fn: (r) => r.{tag_key} == "{tag_value}")\n'
        
        flux_query += f'  |> aggregateWindow(every: {interval}, fn: {aggregation})\n'
        
        return await self.query(flux_query)
    
    def get_capabilities(self) -> Dict:
        """ç²å–èƒ½åŠ›"""
        return {
            'database': 'InfluxDB',
            'version': '2.x',
            'features': {
                'clustering': False,  # é–‹æºç‰ˆä¸æ”¯æ´
                'streaming': True,
                'sql': False,
                'flux': True,
                'compression': True
            },
            'performance': {
                'write_throughput': '200K points/sec',
                'query_latency': 'medium',
                'compression_ratio': '7:1'
            }
        }
    
    async def health_check(self) -> Dict:
        """å¥åº·æª¢æŸ¥"""
        try:
            health = self.client.health()
            return {
                'status': health.status,
                'message': health.message,
                'version': health.version
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': str(e)
            }
```

### 2.2 TDengine é©é…å™¨

```python
# tdengine_adapter.py

import taos
from typing import Dict, List, Any
from datetime import datetime
import logging

class TdengineAdapter(ITSDBAdapter):
    """TDengine é©é…å™¨"""
    
    def __init__(self):
        self.conn: Optional[taos.TaosConnection] = None
        self.cursor = None
        self.database = None
        self.logger = logging.getLogger(__name__)
    
    async def connect(self, config: Dict) -> bool:
        """é€£æ¥ TDengine"""
        try:
            self.conn = taos.connect(
                host=config['host'],
                port=config.get('port', 6030),
                user=config.get('user', 'root'),
                password=config.get('password', 'taosdata'),
                database=config.get('database', 'ndh')
            )
            self.cursor = self.conn.cursor()
            self.database = config.get('database', 'ndh')
            
            # å‰µå»ºæ•¸æ“šåº«(å¦‚æœä¸å­˜åœ¨)
            self.cursor.execute(f"CREATE DATABASE IF NOT EXISTS {self.database}")
            self.cursor.execute(f"USE {self.database}")
            
            self.logger.info("Connected to TDengine successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect to TDengine: {e}")
            raise
    
    async def disconnect(self) -> bool:
        """æ–·é–‹é€£æ¥"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        self.logger.info("Disconnected from TDengine")
        return True
    
    async def write(self, measurement: str, tags: Dict[str, str],
                   fields: Dict[str, Any], timestamp: datetime) -> bool:
        """å¯«å…¥å–®æ¢æ•¸æ“š"""
        try:
            # TDengine ä½¿ç”¨å­è¡¨å = measurement_tagvalue1_tagvalue2...
            subtable_name = f"{measurement}_{'_'.join(tags.values())}"
            
            # æ§‹å»º SQL
            field_names = ', '.join(fields.keys())
            field_values = ', '.join([self._format_value(v) for v in fields.values()])
            tag_values = ', '.join([f"'{v}'" for v in tags.values()])
            
            sql = f"""
            INSERT INTO {subtable_name} USING {measurement} TAGS ({tag_values})
            VALUES ('{timestamp.isoformat()}', {field_values})
            """
            
            self.cursor.execute(sql)
            return True
        except Exception as e:
            self.logger.error(f"Write failed: {e}")
            return False
    
    async def batch_write(self, points: List[Dict]) -> bool:
        """æ‰¹é‡å¯«å…¥"""
        try:
            sql_statements = []
            for point in points:
                measurement = point['measurement']
                tags = point['tags']
                fields = point['fields']
                timestamp = point['timestamp']
                
                subtable_name = f"{measurement}_{'_'.join(tags.values())}"
                field_values = ', '.join([self._format_value(v) for v in fields.values()])
                tag_values = ', '.join([f"'{v}'" for v in tags.values()])
                
                sql = f"{subtable_name} USING {measurement} TAGS ({tag_values}) VALUES ('{timestamp.isoformat()}', {field_values})"
                sql_statements.append(sql)
            
            # TDengine æ”¯æ´æ‰¹é‡æ’å…¥
            batch_sql = "INSERT INTO " + " ".join(sql_statements)
            self.cursor.execute(batch_sql)
            return True
        except Exception as e:
            self.logger.error(f"Batch write failed: {e}")
            return False
    
    async def query(self, query: str, params: Dict = None) -> List[Dict]:
        """åŸ·è¡Œ SQL æŸ¥è©¢"""
        try:
            self.cursor.execute(query)
            columns = [desc[0] for desc in self.cursor.description]
            results = []
            for row in self.cursor.fetchall():
                results.append(dict(zip(columns, row)))
            return results
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            raise
    
    async def query_range(self, measurement: str, tags: Dict,
                         start: datetime, end: datetime,
                         fields: List[str] = None) -> List[Dict]:
        """ç¯„åœæŸ¥è©¢"""
        # æ§‹å»º SQL æŸ¥è©¢
        field_list = ', '.join(fields) if fields else '*'
        sql = f"""
        SELECT ts, {field_list}
        FROM {measurement}
        WHERE ts >= '{start.isoformat()}'
          AND ts < '{end.isoformat()}'
        """
        
        # æ·»åŠ  tag éæ¿¾
        for tag_key, tag_value in tags.items():
            sql += f"  AND {tag_key} = '{tag_value}'\n"
        
        return await self.query(sql)
    
    async def aggregate(self, measurement: str, tags: Dict,
                       start: datetime, end: datetime,
                       aggregation: str, interval: str) -> List[Dict]:
        """èšåˆæŸ¥è©¢"""
        # æ˜ å°„èšåˆå‡½æ•¸
        agg_func_map = {
            'mean': 'AVG',
            'sum': 'SUM',
            'count': 'COUNT',
            'min': 'MIN',
            'max': 'MAX'
        }
        agg_func = agg_func_map.get(aggregation, 'AVG')
        
        sql = f"""
        SELECT _wstart as window_start, {agg_func}(*) as value
        FROM {measurement}
        WHERE ts >= '{start.isoformat()}'
          AND ts < '{end.isoformat()}'
        """
        
        for tag_key, tag_value in tags.items():
            sql += f"  AND {tag_key} = '{tag_value}'\n"
        
        sql += f"INTERVAL({interval})"
        
        return await self.query(sql)
    
    def _format_value(self, value: Any) -> str:
        """æ ¼å¼åŒ–å€¼"""
        if isinstance(value, str):
            return f"'{value}'"
        elif isinstance(value, bool):
            return 'TRUE' if value else 'FALSE'
        else:
            return str(value)
    
    def get_capabilities(self) -> Dict:
        """ç²å–èƒ½åŠ›"""
        return {
            'database': 'TDengine',
            'version': '3.x',
            'features': {
                'clustering': True,
                'streaming': True,
                'sql': True,
                'flux': False,
                'compression': True
            },
            'performance': {
                'write_throughput': '1.5M points/sec',
                'query_latency': 'low',
                'compression_ratio': '16:1'
            }
        }
    
    async def health_check(self) -> Dict:
        """å¥åº·æª¢æŸ¥"""
        try:
            self.cursor.execute("SELECT SERVER_STATUS()")
            result = self.cursor.fetchone()
            return {
                'status': 'healthy' if result else 'unhealthy',
                'database': self.database
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': str(e)
            }
```

### 2.3 TimescaleDB é©é…å™¨

```python
# timescaledb_adapter.py

import asyncpg
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

class TimescaledbAdapter(ITSDBAdapter):
    """TimescaleDB é©é…å™¨"""
    
    def __init__(self):
        self.pool: Optional[asyncpg.Pool] = None
        self.database = None
        self.logger = logging.getLogger(__name__)
    
    async def connect(self, config: Dict) -> bool:
        """é€£æ¥ TimescaleDB"""
        try:
            self.pool = await asyncpg.create_pool(
                host=config['host'],
                port=config.get('port', 5432),
                user=config['user'],
                password=config['password'],
                database=config['database'],
                min_size=config.get('min_connections', 10),
                max_size=config.get('max_connections', 20)
            )
            self.database = config['database']
            
            # æ¸¬è©¦é€£æ¥
            async with self.pool.acquire() as conn:
                version = await conn.fetchval('SELECT version()')
                self.logger.info(f"Connected to TimescaleDB: {version}")
            
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect to TimescaleDB: {e}")
            raise
    
    async def disconnect(self) -> bool:
        """æ–·é–‹é€£æ¥"""
        if self.pool:
            await self.pool.close()
        self.logger.info("Disconnected from TimescaleDB")
        return True
    
    async def write(self, measurement: str, tags: Dict[str, str],
                   fields: Dict[str, Any], timestamp: datetime) -> bool:
        """å¯«å…¥å–®æ¢æ•¸æ“š"""
        try:
            # æ§‹å»º INSERT SQL
            all_columns = ['time'] + list(tags.keys()) + list(fields.keys())
            all_values = [timestamp] + list(tags.values()) + list(fields.values())
            
            placeholders = ', '.join([f'${i+1}' for i in range(len(all_values))])
            columns = ', '.join(all_columns)
            
            sql = f"INSERT INTO {measurement} ({columns}) VALUES ({placeholders})"
            
            async with self.pool.acquire() as conn:
                await conn.execute(sql, *all_values)
            
            return True
        except Exception as e:
            self.logger.error(f"Write failed: {e}")
            return False
    
    async def batch_write(self, points: List[Dict]) -> bool:
        """æ‰¹é‡å¯«å…¥"""
        try:
            # æŒ‰ measurement åˆ†çµ„
            grouped_points = {}
            for point in points:
                measurement = point['measurement']
                if measurement not in grouped_points:
                    grouped_points[measurement] = []
                grouped_points[measurement].append(point)
            
            async with self.pool.acquire() as conn:
                for measurement, points_list in grouped_points.items():
                    # æ§‹å»ºæ‰¹é‡ INSERT
                    first_point = points_list[0]
                    all_columns = ['time'] + list(first_point['tags'].keys()) + list(first_point['fields'].keys())
                    columns = ', '.join(all_columns)
                    
                    values_list = []
                    for point in points_list:
                        values = [point['timestamp']] + list(point['tags'].values()) + list(point['fields'].values())
                        values_list.append(values)
                    
                    # ä½¿ç”¨ COPY é€²è¡Œæ‰¹é‡æ’å…¥(æ›´å¿«)
                    await conn.copy_records_to_table(
                        measurement,
                        columns=all_columns,
                        records=values_list
                    )
            
            return True
        except Exception as e:
            self.logger.error(f"Batch write failed: {e}")
            return False
    
    async def query(self, query: str, params: Dict = None) -> List[Dict]:
        """åŸ·è¡Œ SQL æŸ¥è©¢"""
        try:
            async with self.pool.acquire() as conn:
                if params:
                    rows = await conn.fetch(query, *params.values())
                else:
                    rows = await conn.fetch(query)
                
                results = [dict(row) for row in rows]
                return results
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            raise
    
    async def query_range(self, measurement: str, tags: Dict,
                         start: datetime, end: datetime,
                         fields: List[str] = None) -> List[Dict]:
        """ç¯„åœæŸ¥è©¢"""
        field_list = ', '.join(fields) if fields else '*'
        sql = f"""
        SELECT time, {field_list}
        FROM {measurement}
        WHERE time >= $1 AND time < $2
        """
        
        # æ·»åŠ  tag éæ¿¾
        param_index = 3
        for tag_key, tag_value in tags.items():
            sql += f"  AND {tag_key} = ${param_index}\n"
            param_index += 1
        
        sql += "ORDER BY time"
        
        params = [start, end] + list(tags.values())
        
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(sql, *params)
            return [dict(row) for row in rows]
    
    async def aggregate(self, measurement: str, tags: Dict,
                       start: datetime, end: datetime,
                       aggregation: str, interval: str) -> List[Dict]:
        """èšåˆæŸ¥è©¢"""
        # ä½¿ç”¨ TimescaleDB çš„ time_bucket å‡½æ•¸
        sql = f"""
        SELECT time_bucket('{interval}', time) as bucket,
               {aggregation}(*) as value
        FROM {measurement}
        WHERE time >= $1 AND time < $2
        """
        
        param_index = 3
        for tag_key, tag_value in tags.items():
            sql += f"  AND {tag_key} = ${param_index}\n"
            param_index += 1
        
        sql += "GROUP BY bucket ORDER BY bucket"
        
        params = [start, end] + list(tags.values())
        
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(sql, *params)
            return [dict(row) for row in rows]
    
    def get_capabilities(self) -> Dict:
        """ç²å–èƒ½åŠ›"""
        return {
            'database': 'TimescaleDB',
            'version': '2.x',
            'features': {
                'clustering': True,  # ä¼æ¥­ç‰ˆ
                'streaming': False,
                'sql': True,
                'flux': False,
                'compression': True
            },
            'performance': {
                'write_throughput': '600K points/sec',
                'query_latency': 'medium',
                'compression_ratio': '10:1'
            }
        }
    
    async def health_check(self) -> Dict:
        """å¥åº·æª¢æŸ¥"""
        try:
            async with self.pool.acquire() as conn:
                result = await conn.fetchval('SELECT 1')
                return {
                    'status': 'healthy' if result == 1 else 'unhealthy',
                    'database': self.database
                }
        except Exception as e:
            return {
                'status': 'error',
                'message': str(e)
            }
```

### 2.4 QuestDB é©é…å™¨

```python
# questdb_adapter.py

import asyncpg
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

class QuestdbAdapter(ITSDBAdapter):
    """QuestDB é©é…å™¨"""
    
    def __init__(self):
        self.pool: Optional[asyncpg.Pool] = None
        self.logger = logging.getLogger(__name__)
    
    async def connect(self, config: Dict) -> bool:
        """é€£æ¥ QuestDB (ä½¿ç”¨ PostgreSQL å”è­°)"""
        try:
            self.pool = await asyncpg.create_pool(
                host=config['host'],
                port=config.get('port', 8812),  # QuestDB PostgreSQL port
                user=config.get('user', 'admin'),
                password=config.get('password', 'quest'),
                database=config.get('database', 'qdb'),
                min_size=config.get('min_connections', 10),
                max_size=config.get('max_connections', 20)
            )
            
            # æ¸¬è©¦é€£æ¥
            async with self.pool.acquire() as conn:
                version = await conn.fetchval('SELECT version()')
                self.logger.info(f"Connected to QuestDB: {version}")
            
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect to QuestDB: {e}")
            raise
    
    async def disconnect(self) -> bool:
        """æ–·é–‹é€£æ¥"""
        if self.pool:
            await self.pool.close()
        self.logger.info("Disconnected from QuestDB")
        return True
    
    async def write(self, measurement: str, tags: Dict[str, str],
                   fields: Dict[str, Any], timestamp: datetime) -> bool:
        """å¯«å…¥å–®æ¢æ•¸æ“š"""
        try:
            all_columns = ['timestamp'] + list(tags.keys()) + list(fields.keys())
            all_values = [timestamp] + list(tags.values()) + list(fields.values())
            
            placeholders = ', '.join([f'${i+1}' for i in range(len(all_values))])
            columns = ', '.join(all_columns)
            
            sql = f"INSERT INTO {measurement} ({columns}) VALUES ({placeholders})"
            
            async with self.pool.acquire() as conn:
                await conn.execute(sql, *all_values)
            
            return True
        except Exception as e:
            self.logger.error(f"Write failed: {e}")
            return False
    
    async def batch_write(self, points: List[Dict]) -> bool:
        """æ‰¹é‡å¯«å…¥"""
        # QuestDB å¯¦ç¾é¡ä¼¼ TimescaleDB
        try:
            grouped_points = {}
            for point in points:
                measurement = point['measurement']
                if measurement not in grouped_points:
                    grouped_points[measurement] = []
                grouped_points[measurement].append(point)
            
            async with self.pool.acquire() as conn:
                for measurement, points_list in grouped_points.items():
                    first_point = points_list[0]
                    all_columns = ['timestamp'] + list(first_point['tags'].keys()) + list(first_point['fields'].keys())
                    
                    values_list = []
                    for point in points_list:
                        values = [point['timestamp']] + list(point['tags'].values()) + list(point['fields'].values())
                        values_list.append(values)
                    
                    await conn.copy_records_to_table(
                        measurement,
                        columns=all_columns,
                        records=values_list
                    )
            
            return True
        except Exception as e:
            self.logger.error(f"Batch write failed: {e}")
            return False
    
    async def query(self, query: str, params: Dict = None) -> List[Dict]:
        """åŸ·è¡Œ SQL æŸ¥è©¢"""
        try:
            async with self.pool.acquire() as conn:
                if params:
                    rows = await conn.fetch(query, *params.values())
                else:
                    rows = await conn.fetch(query)
                
                results = [dict(row) for row in rows]
                return results
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            raise
    
    def get_capabilities(self) -> Dict:
        """ç²å–èƒ½åŠ›"""
        return {
            'database': 'QuestDB',
            'version': '7.x',
            'features': {
                'clustering': False,  # ä¸»å¾è¤‡è£½
                'streaming': False,
                'sql': True,
                'flux': False,
                'compression': True
            },
            'performance': {
                'write_throughput': '1.2M points/sec',
                'query_latency': 'low',
                'compression_ratio': '12:1'
            }
        }
    
    async def health_check(self) -> Dict:
        """å¥åº·æª¢æŸ¥"""
        try:
            async with self.pool.acquire() as conn:
                result = await conn.fetchval('SELECT 1')
                return {
                    'status': 'healthy' if result == 1 else 'unhealthy'
                }
        except Exception as e:
            return {
                'status': 'error',
                'message': str(e)
            }
```

---

## 3. é…ç½®ç®¡ç†

### 3.1 é…ç½®æ–‡ä»¶æ ¼å¼

```yaml
# ndh_config.yaml

tsdb:
  # ä¸»æ•¸æ“šåº«é…ç½®
  primary:
    type: timescaledb  # influxdb, tdengine, timescaledb, questdb
    enabled: true
    config:
      host: timescaledb-primary.ndh.svc.cluster.local
      port: 5432
      user: ndh_user
      password: ${TIMESCALEDB_PASSWORD}
      database: ndh
      min_connections: 10
      max_connections: 20
  
  # æ¬¡è¦æ•¸æ“šåº«é…ç½®(ç”¨æ–¼é›™å¯«)
  secondary:
    - type: tdengine
      enabled: false  # å¯é¸çš„é›™å¯«
      config:
        host: tdengine-0.tdengine.ndh.svc.cluster.local
        port: 6030
        user: root
        password: ${TDENGINE_PASSWORD}
        database: ndh
  
  # é©é…å™¨é…ç½®
  adapters:
    influxdb:
      module: ndh.adapters.influxdb_adapter
      class: InfluxdbAdapter
    tdengine:
      module: ndh.adapters.tdengine_adapter
      class: TdengineAdapter
    timescaledb:
      module: ndh.adapters.timescaledb_adapter
      class: TimescaledbAdapter
    questdb:
      module: ndh.adapters.questdb_adapter
      class: QuestdbAdapter
  
  # é›™å¯«é…ç½®
  dual_write:
    enabled: false
    async: true  # éé˜»å¡å¯«å…¥æ¬¡è¦æ•¸æ“šåº«
    retry: 3
    timeout: 5  # seconds
  
  # åˆ‡æ›é…ç½®
  switchover:
    enabled: true
    validation: true  # åˆ‡æ›å‰é©—è­‰æ•¸æ“šä¸€è‡´æ€§
    rollback: true  # æ”¯æ´å›æ»¾
```

### 3.2 é…ç½®è¼‰å…¥å™¨

```python
# config_loader.py

import yaml
import os
from typing import Dict

class TSDBConfigLoader:
    """æ™‚åºæ•¸æ“šåº«é…ç½®è¼‰å…¥å™¨"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # æ›¿æ›ç’°å¢ƒè®Šæ•¸
        config = TSDBConfigLoader._replace_env_vars(config)
        
        return config['tsdb']
    
    @staticmethod
    def _replace_env_vars(config: Dict) -> Dict:
        """æ›¿æ›é…ç½®ä¸­çš„ç’°å¢ƒè®Šæ•¸"""
        if isinstance(config, dict):
            return {k: TSDBConfigLoader._replace_env_vars(v) for k, v in config.items()}
        elif isinstance(config, list):
            return [TSDBConfigLoader._replace_env_vars(item) for item in config]
        elif isinstance(config, str) and config.startswith('${') and config.endswith('}'):
            env_var = config[2:-1]
            return os.getenv(env_var, config)
        else:
            return config
```

---

## 4. åœ¨ç·šé·ç§»æ–¹æ¡ˆ

### 4.1 é·ç§»æµç¨‹

```
éšæ®µ 1: æº–å‚™éšæ®µ
  â”œâ”€â”€ éƒ¨ç½²æ–°æ•¸æ“šåº«
  â”œâ”€â”€ é…ç½®é›™å¯«
  â””â”€â”€ é©—è­‰é€£æ¥

éšæ®µ 2: é›™å¯«éšæ®µ
  â”œâ”€â”€ å•Ÿç”¨é›™å¯«(èˆŠæ•¸æ“šåº« + æ–°æ•¸æ“šåº«)
  â”œâ”€â”€ ç›£æ§æ•¸æ“šä¸€è‡´æ€§
  â””â”€â”€ é·ç§»æ­·å²æ•¸æ“š

éšæ®µ 3: é©—è­‰éšæ®µ
  â”œâ”€â”€ å°æ¯”æ•¸æ“š
  â”œâ”€â”€ æ€§èƒ½æ¸¬è©¦
  â””â”€â”€ åˆ‡æ›è®€å–åˆ°æ–°æ•¸æ“šåº«

éšæ®µ 4: åˆ‡æ›éšæ®µ
  â”œâ”€â”€ åˆ‡æ›ä¸»æ•¸æ“šåº«
  â”œâ”€â”€ åœæ­¢å¯«å…¥èˆŠæ•¸æ“šåº«
  â””â”€â”€ ç›£æ§ç©©å®šæ€§

éšæ®µ 5: æ¸…ç†éšæ®µ
  â”œâ”€â”€ ä¿ç•™èˆŠæ•¸æ“šåº« N å¤©
  â”œâ”€â”€ ç¢ºèªç„¡å•é¡Œ
  â””â”€â”€ ä¸‹ç·šèˆŠæ•¸æ“šåº«
```

### 4.2 é·ç§»å·¥å…·

```python
# migration_tool.py

from typing import Dict, List
from datetime import datetime, timedelta
import asyncio
import logging

class TSDBMigrationTool:
    """æ™‚åºæ•¸æ“šåº«é·ç§»å·¥å…·"""
    
    def __init__(self, adapter_manager: TSDBAdapterManager):
        self.adapter_manager = adapter_manager
        self.logger = logging.getLogger(__name__)
    
    async def enable_dual_write(self, target_adapter_type: str):
        """å•Ÿç”¨é›™å¯«"""
        self.logger.info(f"Enabling dual write to {target_adapter_type}")
        
        # è¼‰å…¥ç›®æ¨™é©é…å™¨
        if target_adapter_type not in self.adapter_manager.adapters:
            await self.adapter_manager._load_adapter(
                target_adapter_type,
                self.adapter_manager.config['secondary'][0]
            )
        
        # æ·»åŠ åˆ°æ¬¡è¦é©é…å™¨åˆ—è¡¨
        target_adapter = self.adapter_manager.adapters[target_adapter_type]
        if target_adapter not in self.adapter_manager.secondary_adapters:
            self.adapter_manager.secondary_adapters.append(target_adapter)
        
        self.logger.info("Dual write enabled")
    
    async def migrate_historical_data(self, 
                                     source_adapter: ITSDBAdapter,
                                     target_adapter: ITSDBAdapter,
                                     start_date: datetime,
                                     end_date: datetime,
                                     measurements: List[str],
                                     batch_size: int = 10000):
        """é·ç§»æ­·å²æ•¸æ“š"""
        self.logger.info(f"Starting historical data migration from {start_date} to {end_date}")
        
        total_points = 0
        for measurement in measurements:
            self.logger.info(f"Migrating measurement: {measurement}")
            
            current_date = start_date
            while current_date < end_date:
                batch_end = min(current_date + timedelta(hours=1), end_date)
                
                # å¾æºæ•¸æ“šåº«è®€å–
                data = await source_adapter.query_range(
                    measurement=measurement,
                    tags={},
                    start=current_date,
                    end=batch_end
                )
                
                if data:
                    # è½‰æ›ç‚ºæ‰¹é‡å¯«å…¥æ ¼å¼
                    points = []
                    for row in data:
                        point = {
                            'measurement': measurement,
                            'timestamp': row['time'],
                            'tags': {k: v for k, v in row.items() if k.startswith('tag_')},
                            'fields': {k: v for k, v in row.items() if k.startswith('field_')}
                        }
                        points.append(point)
                    
                    # å¯«å…¥ç›®æ¨™æ•¸æ“šåº«
                    await target_adapter.batch_write(points)
                    total_points += len(points)
                    
                    self.logger.info(f"Migrated {len(points)} points, total: {total_points}")
                
                current_date = batch_end
                
                # é¿å…éè¼‰
                await asyncio.sleep(0.1)
        
        self.logger.info(f"Migration completed. Total points migrated: {total_points}")
        return total_points
    
    async def validate_data_consistency(self,
                                       source_adapter: ITSDBAdapter,
                                       target_adapter: ITSDBAdapter,
                                       measurement: str,
                                       start: datetime,
                                       end: datetime) -> Dict:
        """é©—è­‰æ•¸æ“šä¸€è‡´æ€§"""
        self.logger.info(f"Validating data consistency for {measurement}")
        
        # å¾å…©å€‹æ•¸æ“šåº«è®€å–ç›¸åŒç¯„åœçš„æ•¸æ“š
        source_data = await source_adapter.query_range(
            measurement=measurement,
            tags={},
            start=start,
            end=end
        )
        
        target_data = await target_adapter.query_range(
            measurement=measurement,
            tags={},
            start=start,
            end=end
        )
        
        # æ¯”è¼ƒæ•¸æ“šé‡
        source_count = len(source_data)
        target_count = len(target_data)
        
        consistency_ratio = target_count / source_count if source_count > 0 else 0
        
        result = {
            'measurement': measurement,
            'source_count': source_count,
            'target_count': target_count,
            'consistency_ratio': consistency_ratio,
            'status': 'pass' if consistency_ratio >= 0.99 else 'fail'
        }
        
        self.logger.info(f"Validation result: {result}")
        return result
    
    async def switchover(self, new_primary_type: str):
        """åˆ‡æ›ä¸»æ•¸æ“šåº«"""
        self.logger.info(f"Starting switchover to {new_primary_type}")
        
        # é©—è­‰æ–°æ•¸æ“šåº«å¥åº·ç‹€æ…‹
        new_adapter = self.adapter_manager.adapters[new_primary_type]
        health = await new_adapter.health_check()
        
        if health['status'] != 'healthy':
            raise Exception(f"New primary database is not healthy: {health}")
        
        # åŸ·è¡Œåˆ‡æ›
        await self.adapter_manager.switch_primary(new_primary_type)
        
        self.logger.info("Switchover completed successfully")
    
    async def disable_dual_write(self, adapter_type: str):
        """åœæ­¢é›™å¯«"""
        self.logger.info(f"Disabling dual write to {adapter_type}")
        
        adapter = self.adapter_manager.adapters[adapter_type]
        if adapter in self.adapter_manager.secondary_adapters:
            self.adapter_manager.secondary_adapters.remove(adapter)
        
        self.logger.info("Dual write disabled")
```

---

## 5. API å±¤æ•´åˆ

### 5.1 RESTful API

```python
# api.py

from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Dict, Optional
from datetime import datetime

app = FastAPI(title="NDH Time Series API")

# å…¨å±€é©é…å™¨ç®¡ç†å™¨
adapter_manager: Optional[TSDBAdapterManager] = None

class WriteRequest(BaseModel):
    measurement: str
    tags: Dict[str, str]
    fields: Dict[str, float]
    timestamp: Optional[datetime] = None

class QueryRequest(BaseModel):
    measurement: str
    tags: Dict[str, str] = {}
    start: datetime
    end: datetime
    fields: Optional[List[str]] = None

class AggregateRequest(BaseModel):
    measurement: str
    tags: Dict[str, str] = {}
    start: datetime
    end: datetime
    aggregation: str = "mean"
    interval: str = "1h"

@app.on_event("startup")
async def startup():
    """å•Ÿå‹•æ™‚åˆå§‹åŒ–é©é…å™¨ç®¡ç†å™¨"""
    global adapter_manager
    config = TSDBConfigLoader.load_config("ndh_config.yaml")
    adapter_manager = TSDBAdapterManager(config)
    await adapter_manager.initialize()

@app.post("/api/v1/timeseries/write")
async def write_data(request: WriteRequest):
    """å¯«å…¥æ™‚åºæ•¸æ“š"""
    timestamp = request.timestamp or datetime.utcnow()
    
    success = await adapter_manager.write(
        measurement=request.measurement,
        tags=request.tags,
        fields=request.fields,
        timestamp=timestamp
    )
    
    if success:
        return {"status": "success"}
    else:
        raise HTTPException(status_code=500, detail="Write failed")

@app.post("/api/v1/timeseries/query")
async def query_data(request: QueryRequest):
    """æŸ¥è©¢æ™‚åºæ•¸æ“š"""
    try:
        data = await adapter_manager.primary_adapter.query_range(
            measurement=request.measurement,
            tags=request.tags,
            start=request.start,
            end=request.end,
            fields=request.fields
        )
        return {"data": data}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/timeseries/aggregate")
async def aggregate_data(request: AggregateRequest):
    """èšåˆæŸ¥è©¢"""
    try:
        data = await adapter_manager.primary_adapter.aggregate(
            measurement=request.measurement,
            tags=request.tags,
            start=request.start,
            end=request.end,
            aggregation=request.aggregation,
            interval=request.interval
        )
        return {"data": data}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/timeseries/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    health = await adapter_manager.health_check()
    return health

@app.get("/api/v1/timeseries/capabilities")
async def get_capabilities():
    """ç²å–ç•¶å‰æ•¸æ“šåº«èƒ½åŠ›"""
    capabilities = adapter_manager.primary_adapter.get_capabilities()
    return capabilities

@app.post("/api/v1/admin/switchover")
async def switchover(new_primary: str):
    """åˆ‡æ›ä¸»æ•¸æ“šåº«"""
    try:
        migration_tool = TSDBMigrationTool(adapter_manager)
        await migration_tool.switchover(new_primary)
        return {"status": "success", "new_primary": new_primary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## 6. éƒ¨ç½²é…ç½®

### 6.1 Kubernetes éƒ¨ç½²

```yaml
# ndh-api-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ndh-api
  namespace: ndh
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ndh-api
  template:
    metadata:
      labels:
        app: ndh-api
    spec:
      containers:
      - name: ndh-api
        image: ndh/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: TIMESCALEDB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: ndh-secrets
              key: timescaledb-password
        - name: TDENGINE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: ndh-secrets
              key: tdengine-password
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: config
        configMap:
          name: ndh-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ndh-config
  namespace: ndh
data:
  ndh_config.yaml: |
    tsdb:
      primary:
        type: timescaledb
        enabled: true
        config:
          host: timescaledb-primary.ndh.svc.cluster.local
          port: 5432
          user: ndh_user
          password: ${TIMESCALEDB_PASSWORD}
          database: ndh
      secondary:
        - type: tdengine
          enabled: false
          config:
            host: tdengine-0.tdengine.ndh.svc.cluster.local
            port: 6030
            user: root
            password: ${TDENGINE_PASSWORD}
            database: ndh
```

---

## 7. æ¸¬è©¦èˆ‡é©—è­‰

### 7.1 å–®å…ƒæ¸¬è©¦

```python
# test_adapters.py

import pytest
from datetime import datetime
import asyncio

@pytest.mark.asyncio
async def test_influxdb_adapter():
    """æ¸¬è©¦ InfluxDB é©é…å™¨"""
    adapter = InfluxdbAdapter()
    
    config = {
        'url': 'http://localhost:8086',
        'token': 'test-token',
        'org': 'test-org',
        'bucket': 'test-bucket'
    }
    
    # é€£æ¥
    await adapter.connect(config)
    
    # å¯«å…¥
    success = await adapter.write(
        measurement='test',
        tags={'location': 'zone_a'},
        fields={'temperature': 25.5},
        timestamp=datetime.utcnow()
    )
    assert success
    
    # æŸ¥è©¢
    data = await adapter.query_range(
        measurement='test',
        tags={'location': 'zone_a'},
        start=datetime.utcnow() - timedelta(hours=1),
        end=datetime.utcnow()
    )
    assert len(data) > 0
    
    # æ–·é–‹
    await adapter.disconnect()

@pytest.mark.asyncio
async def test_adapter_manager():
    """æ¸¬è©¦é©é…å™¨ç®¡ç†å™¨"""
    config = {
        'primary': {
            'type': 'influxdb',
            'config': {...}
        }
    }
    
    manager = TSDBAdapterManager(config)
    await manager.initialize()
    
    # å¯«å…¥
    success = await manager.write(
        measurement='test',
        tags={'location': 'zone_a'},
        fields={'temperature': 25.5},
        timestamp=datetime.utcnow()
    )
    assert success
    
    # å¥åº·æª¢æŸ¥
    health = await manager.health_check()
    assert health['primary']['status'] == 'healthy'
```

### 7.2 æ€§èƒ½æ¸¬è©¦

```python
# benchmark.py

import asyncio
import time
from datetime import datetime
import random

async def benchmark_write(adapter: ITSDBAdapter, num_points: int):
    """å¯«å…¥æ€§èƒ½æ¸¬è©¦"""
    start_time = time.time()
    
    points = []
    for i in range(num_points):
        point = {
            'measurement': 'benchmark',
            'timestamp': datetime.utcnow(),
            'tags': {
                'equipment_id': f'EQ_{i % 100}',
                'location': f'Zone_{i % 10}'
            },
            'fields': {
                'temperature': random.uniform(20, 80),
                'pressure': random.uniform(1, 10),
                'throughput': random.uniform(1000, 15000)
            }
        }
        points.append(point)
    
    await adapter.batch_write(points)
    
    end_time = time.time()
    duration = end_time - start_time
    throughput = num_points / duration
    
    print(f"Write Performance:")
    print(f"  Points: {num_points}")
    print(f"  Duration: {duration:.2f}s")
    print(f"  Throughput: {throughput:.0f} points/sec")

async def benchmark_query(adapter: ITSDBAdapter):
    """æŸ¥è©¢æ€§èƒ½æ¸¬è©¦"""
    start_time = time.time()
    
    data = await adapter.query_range(
        measurement='benchmark',
        tags={'equipment_id': 'EQ_1'},
        start=datetime.utcnow() - timedelta(hours=24),
        end=datetime.utcnow()
    )
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"Query Performance:")
    print(f"  Points returned: {len(data)}")
    print(f"  Duration: {duration:.2f}s")
```

---

## 8. æœ€ä½³å¯¦è¸

### 8.1 é¸æ“‡é©é…å™¨çš„å»ºè­°

| å ´åŸŸ | æ¨è–¦æ•¸æ“šåº« | ç†ç”± |
|------|-----------|------|
| **é«˜æ•æ„Ÿ** (å°ç©é›») | TimescaleDB | ä¾›æ‡‰éˆé¢¨éšªæœ€ä½,ç¬¦åˆå®¢æˆ¶è¦æ±‚ |
| **ä¸­æ•æ„Ÿ** (ä¸€èˆ¬è£½é€ æ¥­) | QuestDB | æ€§èƒ½å„ªç•°,æˆæœ¬ä½,é¢¨éšªå¯æ¥å— |
| **ä½æ•æ„Ÿ** (éé—œéµæ‡‰ç”¨) | TDengine | æ€§èƒ½æœ€ä½³,æˆæœ¬æœ€ä½ |
| **é–‹ç™¼æ¸¬è©¦** | InfluxDB | ç”Ÿæ…‹æˆç†Ÿ,æ˜“æ–¼ä½¿ç”¨ |

### 8.2 é·ç§»å»ºè­°

1. **å……åˆ†æ¸¬è©¦**: åœ¨æ¸¬è©¦ç’°å¢ƒå®Œæ•´é©—è­‰
2. **é€æ­¥é·ç§»**: å…ˆé›™å¯«,å†åˆ‡æ›è®€å–,æœ€å¾Œåˆ‡æ›å¯«å…¥
3. **ç›£æ§å‘Šè­¦**: å¯¦æ™‚ç›£æ§æ•¸æ“šä¸€è‡´æ€§å’Œæ€§èƒ½
4. **å›æ»¾è¨ˆç•«**: æº–å‚™å¿«é€Ÿå›æ»¾æ–¹æ¡ˆ
5. **ä¿ç•™èˆŠæ•¸æ“š**: è‡³å°‘ä¿ç•™ 30 å¤©ä½œç‚ºå‚™ä»½

### 8.3 æ€§èƒ½å„ªåŒ–

1. **æ‰¹é‡å¯«å…¥**: ä½¿ç”¨ batch_write è€Œéå–®æ¢å¯«å…¥
2. **é€£æ¥æ± **: ä½¿ç”¨é€£æ¥æ± ç®¡ç†æ•¸æ“šåº«é€£æ¥
3. **ç•°æ­¥ I/O**: ä½¿ç”¨ asyncio æé«˜ä¸¦ç™¼æ€§èƒ½
4. **é©ç•¶ç´¢å¼•**: ç‚ºå¸¸ç”¨æŸ¥è©¢å­—æ®µå‰µå»ºç´¢å¼•
5. **æ•¸æ“šåˆ†å€**: ä½¿ç”¨æ™‚é–“åˆ†å€å„ªåŒ–æŸ¥è©¢

---

## 9. ç¸½çµ

### 9.1 æ ¸å¿ƒå„ªå‹¢

âœ… **éˆæ´»æ€§**: æ ¹æ“šå ´åŸŸéœ€æ±‚é¸æ“‡æœ€é©åˆçš„æ•¸æ“šåº«  
âœ… **é¢¨éšªç®¡ç†**: é™ä½ä¾›æ‡‰éˆé¢¨éšª,é¿å…å» å•†é–å®š  
âœ… **å¹³æ»‘é·ç§»**: æ”¯æ´é›¶åœæ©Ÿåœ¨ç·šé·ç§»  
âœ… **æˆæœ¬å„ªåŒ–**: åœ¨æ€§èƒ½ã€æˆæœ¬ã€å®‰å…¨ä¹‹é–“éˆæ´»å¹³è¡¡  
âœ… **å¯æ“´å±•**: è¼•é¬†æ·»åŠ æ–°çš„æ•¸æ“šåº«é©é…å™¨

### 9.2 å¯¦æ–½å»ºè­°

1. **ç«‹å³**: å¯¦ç¾æŠ½è±¡å±¤å’Œé©é…å™¨ç®¡ç†å™¨
2. **çŸ­æœŸ**: å¯¦ç¾ InfluxDB å’Œ TimescaleDB é©é…å™¨
3. **ä¸­æœŸ**: å¯¦ç¾ TDengine å’Œ QuestDB é©é…å™¨
4. **é•·æœŸ**: æ”¯æ´æ›´å¤šæ•¸æ“šåº«,å®Œå–„é·ç§»å·¥å…·

### 9.3 é æœŸæ•ˆç›Š

- ğŸ¯ **é™ä½é¢¨éšª**: ä¸ä¾è³´å–®ä¸€æ•¸æ“šåº«ä¾›æ‡‰å•†
- ğŸ¯ **æé«˜éˆæ´»æ€§**: æ ¹æ“šéœ€æ±‚å¿«é€Ÿåˆ‡æ›
- ğŸ¯ **å„ªåŒ–æˆæœ¬**: é¸æ“‡æ€§åƒ¹æ¯”æœ€é«˜çš„æ–¹æ¡ˆ
- ğŸ¯ **ä¿è­‰å®‰å…¨**: åœ¨æ•æ„Ÿå ´åŸŸä½¿ç”¨å®‰å…¨çš„æ•¸æ“šåº«

---

**æ–‡æª”çµæŸ**

**ä½œè€…**: æ—å¿—éŒš (Chih Cheng Lin, Michael Lin)  
**Email**: chchlin1018@gmail.com  
**ç‰ˆæœ¬**: 1.0  
**æ—¥æœŸ**: 2025 å¹´ 10 æœˆ 12 æ—¥

